# AI Model Testing Framework - Copilot Instructions

This workspace contains a comprehensive AI/ML model testing framework designed for AI QA professionals.

## Project Overview
A professional-grade testing framework for validating LLMs, ML models, and AI systems with focus on quality assurance, bias detection, and performance benchmarking.

## Key Components
- LLM Testing: Prompt validation, response evaluation, hallucination detection
- ML Model QA: Performance metrics, regression testing, model comparison
- Bias & Fairness: Automated bias detection across demographics
- Performance Testing: Latency, throughput, resource usage benchmarks
- Integration Testing: End-to-end AI system validation
- Test Management: Organized test suites with pytest integration
- Reporting: Comprehensive dashboards and metrics

## Development Guidelines
- Use pytest for all test implementations
- Follow type hints for better code quality
- Keep test cases modular and reusable
- Document all evaluation metrics clearly
- Support multiple AI providers (OpenAI, Anthropic, HuggingFace)
